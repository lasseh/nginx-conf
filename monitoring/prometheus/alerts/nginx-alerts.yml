# =============================================================================
# NGINX PROMETHEUS ALERT RULES
# =============================================================================
# Alerts for nginx monitoring (open-source nginx — no NGINX Plus required)
#
# Two metric sources:
#
# 1. stub_status + nginx-prometheus-exporter (Layer 1):
#    nginx_connections_active    - current active connections
#    nginx_connections_reading   - connections reading request
#    nginx_connections_writing   - connections writing response
#    nginx_connections_waiting   - idle keepalive connections
#    nginx_connections_accepted  - total accepted (counter)
#    nginx_connections_handled   - total handled (counter)
#    nginx_http_requests_total   - total requests (counter, no status label)
#
# 2. Grafana Alloy log-derived metrics (Layer 2):
#    nginx_http_requests_by_status_total     - requests by method + status_class
#    nginx_http_request_duration_seconds     - response time histogram
#    nginx_http_response_bytes_total         - total response bytes
#
# Installation:
#   1. Copy to prometheus rules directory: /etc/prometheus/rules/
#   2. Add to prometheus.yml: rule_files: - 'rules/nginx-alerts.yml'
#   3. Reload prometheus: curl -X POST http://localhost:9090/-/reload
#
# =============================================================================

groups:
  - name: nginx_alerts
    interval: 30s
    rules:
      # =========================================================================
      # AVAILABILITY ALERTS
      # =========================================================================
      - alert: NginxDown
        expr: up{job="nginx"} == 0
        for: 1m
        labels:
          severity: critical
          component: nginx
        annotations:
          summary: "Nginx is down on {{ $labels.instance }}"
          description: "Nginx exporter on {{ $labels.instance }} has been down for more than 1 minute."
          runbook: "Check nginx service: systemctl status nginx"

      # =========================================================================
      # CONNECTION ALERTS (from stub_status)
      # =========================================================================
      - alert: NginxHighConnections
        expr: nginx_connections_active > 1000
        for: 5m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "High number of active connections on {{ $labels.instance }}"
          description: "{{ $labels.instance }} has {{ $value }} active connections (threshold: 1000)"
          runbook: "Check for DDoS attack or traffic spike. Review rate limiting."

      - alert: NginxHighWaitingConnections
        expr: nginx_connections_waiting > 500
        for: 5m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "High number of waiting connections on {{ $labels.instance }}"
          description: "{{ $labels.instance }} has {{ $value }} connections waiting (threshold: 500)"
          runbook: "Check keepalive settings and backend response times"

      - alert: NginxDroppedConnections
        expr: rate(nginx_connections_accepted[5m]) - rate(nginx_connections_handled[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "Nginx is dropping connections on {{ $labels.instance }}"
          description: "{{ $labels.instance }} is dropping {{ $value }} connections/s"
          runbook: "Increase worker_connections or worker_processes in nginx.conf"

      # =========================================================================
      # REQUEST RATE ALERTS (from stub_status)
      # =========================================================================
      - alert: NginxHighRequestRate
        expr: rate(nginx_http_requests_total[5m]) > 1000
        for: 5m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "High request rate on {{ $labels.instance }}"
          description: "{{ $labels.instance }} is handling {{ $value }} req/s (threshold: 1000)"
          runbook: "Check if traffic is legitimate. Consider scaling or rate limiting."

      - alert: NginxRequestRateDrop
        expr: rate(nginx_http_requests_total[5m]) < 10 and rate(nginx_http_requests_total[5m] offset 1h) > 100
        for: 10m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "Request rate dropped significantly on {{ $labels.instance }}"
          description: "{{ $labels.instance }} request rate dropped to {{ $value }} req/s (was >100 an hour ago)"
          runbook: "Check if application is down or if there's a DNS/routing issue"

      # =========================================================================
      # ERROR RATE ALERTS (from Alloy log-derived metrics)
      # =========================================================================
      # Requires Grafana Alloy — see monitoring/alloy/config.alloy

      - alert: NginxHigh5xxRate
        expr: |
          sum(rate(nginx_http_requests_by_status_total{status_class="5xx"}[5m]))
          / sum(rate(nginx_http_requests_by_status_total[5m])) * 100 > 5
        for: 5m
        labels:
          severity: critical
          component: nginx
        annotations:
          summary: "High 5xx error rate on nginx"
          description: "5xx error rate is {{ $value | printf \"%.1f\" }}% (threshold: 5%)"
          runbook: "Check upstream backends: are they healthy? Check error logs for details."

      - alert: NginxHigh4xxRate
        expr: |
          sum(rate(nginx_http_requests_by_status_total{status_class="4xx"}[5m]))
          / sum(rate(nginx_http_requests_by_status_total[5m])) * 100 > 25
        for: 10m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "High 4xx error rate on nginx"
          description: "4xx error rate is {{ $value | printf \"%.1f\" }}% (threshold: 25%)"
          runbook: "Check access logs for bad client requests, broken links, or scanning attempts."

      # =========================================================================
      # LATENCY ALERTS (from Alloy log-derived metrics)
      # =========================================================================

      - alert: NginxSlowResponses
        expr: |
          histogram_quantile(0.95,
            sum(rate(nginx_http_request_duration_seconds_bucket[5m])) by (le)
          ) > 2
        for: 10m
        labels:
          severity: warning
          component: nginx
        annotations:
          summary: "Nginx p95 response time is high"
          description: "p95 response time is {{ $value | printf \"%.2f\" }}s (threshold: 2s)"
          runbook: "Check upstream backend performance. Look for slow queries or resource exhaustion."

      - alert: NginxVerySlowResponses
        expr: |
          histogram_quantile(0.99,
            sum(rate(nginx_http_request_duration_seconds_bucket[5m])) by (le)
          ) > 10
        for: 5m
        labels:
          severity: critical
          component: nginx
        annotations:
          summary: "Nginx p99 response time is very high"
          description: "p99 response time is {{ $value | printf \"%.2f\" }}s (threshold: 10s)"
          runbook: "Investigate upstream timeouts. Check for resource exhaustion or deadlocks."
